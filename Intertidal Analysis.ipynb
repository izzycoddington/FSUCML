{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intertidal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "# basic df\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# modelling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn import tree\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# map\n",
    "import folium\n",
    "from folium.plugins import HeatMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intertidal coordinates\n",
    "intertidal_coords = pd.read_excel('C:\\\\Users\\\\izzyc\\\\Downloads\\\\IntertidalCoordinates (1).xlsx', sheet_name='Sheet1')\n",
    "# condition\n",
    "intertidal_condition = pd.read_excel(\"C:\\\\Users\\\\izzyc\\\\Downloads\\\\Intertidal_Oyster_Condition_Dec_2019-July_2023.xlsx\", sheet_name='Intertidal_Condition')\n",
    "# water quality\n",
    "intertidal_wq = pd.read_excel(\"C:\\\\Users\\\\izzyc\\\\Downloads\\\\Intertidal_Oyster_Condition_WQ_Dec_2019-July_2023.xlsx\", sheet_name='Water_Quality')\n",
    "# disease\n",
    "intertidal_disease = pd.read_excel(\"C:\\\\Users\\\\izzyc\\\\Downloads\\\\Interdital_Oyster_Disease_July_2021-February_2023.xlsx\", sheet_name = 'Disease_Prevelance')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coordinates\n",
    "coords = intertidal_coords\n",
    "\n",
    "\n",
    "# condition\n",
    "intertidal_condition['SiteID'] = intertidal_condition['Site'].astype(str) + '-' + intertidal_condition['Reef'].astype(str)\n",
    "condition_voi = ['SiteID','Reef','Date_Sample', 'Oyster_Number', 'Height_mm', 'Length_mm', 'Width_mm',\n",
    "                 'Total_Weight_g', 'Soft_Tiss_Wet', 'Shell_Wet', 'Soft_Tiss_Dry', 'Shell_Dry', 'Sex']\n",
    "condition = intertidal_condition[condition_voi]\n",
    "\n",
    "\n",
    "# water quality\n",
    "intertidal_wq['SiteID'] = intertidal_wq['Site'].astype(str) + '-' + intertidal_wq['Reef'].astype(str)\n",
    "wq_voi = ['SiteID', 'Date_Collected', 'Reef', 'Time', 'Ambient_Temp_°C', 'Water_Temp_°C'\n",
    "          ,'Salinity_PPT' ]\n",
    "wq = intertidal_wq[wq_voi]\n",
    "\n",
    "# disease\n",
    "intertidal_disease['SiteID'] = intertidal_disease['Site'].astype(str) + '-' + intertidal_disease['Reef'].astype(str)\n",
    "disease_voi = ['SiteID', 'Reef','Date_Collected', 'Oyster_Number', 'Disease_Scale#']\n",
    "disease = intertidal_disease[disease_voi]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Dataset with Variables of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step one. Merge coord and condition on SiteID\n",
    "merged_df = pd.merge(condition, coords, on='SiteID')\n",
    "\n",
    "# Step two. Merge with water quality on siteID, date collected\n",
    "merged_df = pd.merge(merged_df, wq, left_on=['SiteID', 'Date_Sample'], right_on=['SiteID', 'Date_Collected'])\n",
    "\n",
    "# Step three. Merge with disease on SiteID, Date Collected, Oyster Number\n",
    "merged_df = pd.merge(merged_df, disease, on=['SiteID', 'Oyster_Number'])\n",
    "\n",
    "# Drop duplicate columns (if any)\n",
    "merged_df = merged_df.loc[:,~merged_df.columns.duplicated()]\n",
    "\n",
    "# Display the resulting dataframe\n",
    "merged_df.info()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop(columns=[ 'Reef_y', 'Site'])\n",
    "merged_df['Date_Collected'] = merged_df['Date_Collected_y']\n",
    "merged_df['Disease_Scale'] = merged_df['Disease_Scale#']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning: Converting Objects to Float and Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert necessary attributes to float\n",
    "merged_df['Height_mm'] = pd.to_numeric(merged_df['Height_mm'], errors='coerce')\n",
    "merged_df['Length_mm'] = pd.to_numeric(merged_df['Length_mm'], errors='coerce')\n",
    "merged_df['Width_mm'] = pd.to_numeric(merged_df['Width_mm'], errors='coerce')\n",
    "merged_df['Total_Weight_g'] = pd.to_numeric(merged_df['Total_Weight_g'], errors='coerce')\n",
    "merged_df['Soft_Tiss_Wet'] = pd.to_numeric(merged_df['Soft_Tiss_Wet'], errors='coerce')\n",
    "merged_df['Shell_Wet'] = pd.to_numeric(merged_df['Shell_Wet'], errors='coerce')\n",
    "merged_df['Soft_Tiss_Dry'] = pd.to_numeric(merged_df['Soft_Tiss_Dry'], errors='coerce')\n",
    "merged_df['Shell_Dry'] = pd.to_numeric(merged_df['Shell_Dry'], errors='coerce')\n",
    "merged_df['Ambient_Temp_°C'] = pd.to_numeric(merged_df['Ambient_Temp_°C'], errors='coerce')\n",
    "merged_df['Water_Temp_°C'] = pd.to_numeric(merged_df['Water_Temp_°C'], errors='coerce')\n",
    "merged_df['Salinity_PPT'] = pd.to_numeric(merged_df['Salinity_PPT'], errors='coerce')\n",
    "merged_df['Disease_Scale'] = pd.to_numeric(merged_df['Disease_Scale'], errors='coerce')\n",
    "\n",
    "# now convert time and date_collected to datetime\n",
    "# first replace ND with naT (not a time)\n",
    "merged_df['Date_Collected'] = merged_df['Date_Collected'].replace('ND', pd.NaT)\n",
    "merged_df['Date_Sample'] = merged_df['Date_Sample'].replace('ND', pd.NaT)\n",
    "merged_df['Time'] = merged_df['Time'].replace('ND', pd.NaT)\n",
    "merged_df['Date_Collected'] = pd.to_datetime(merged_df['Date_Collected'], errors='coerce')\n",
    "merged_df['Time'] = pd.to_datetime(merged_df['Time'], errors='coerce')\n",
    "merged_df['Date_Sample'] = pd.to_datetime(merged_df['Date_Sample'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoding Categorical to Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, encode SiteID and sex to levels\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# transform site and sex\n",
    "merged_df['SiteID_encoded'] = label_encoder.fit_transform(merged_df['SiteID'])\n",
    "merged_df['Sex_encoded'] = label_encoder.fit_transform(merged_df['Sex'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.drop(columns=['Disease_Scale#', 'Reef'])\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['Disease_Scale'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap of missing values\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(merged_df.isnull(), cmap='viridis', cbar=False, yticklabels=False)\n",
    "\n",
    "# Set plot title\n",
    "plt.title('Missing Values in the Dataset')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix\n",
    "correlation_matrix = merged_df.corr()\n",
    "\n",
    "# Seaborn heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Correlation Matrix of Predictors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time vs. Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize size over time per site\n",
    "# visualize height over time\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(merged_df['Date_Sample'], merged_df['Height_mm'], c = merged_df['SiteID_encoded'], cmap = 'viridis')\n",
    "plt.colorbar(label='SiteID')\n",
    "plt.xlabel('Date Sampled')\n",
    "plt.ylabel('Height in mm')\n",
    "plt.title('Oyster Height over time')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Size By SiteID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of size per site\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(x='SiteID', y='Height_mm', data=merged_df, palette='Set2')\n",
    "plt.xlabel('Site ID')\n",
    "plt.ylabel('Height (mm)')\n",
    "plt.title('Violin Plot of Height by Site')\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seasonal Salinity and Water Temp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.scatter(merged_df['Date_Sample'], merged_df['Salinity_PPT'], marker='o' )       # replace with water temp for alternate viz\n",
    "plt.xlabel('Sample Date')\n",
    "plt.ylabel('Salinity PPT')\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Binary Indicator of Dermo for Prediction Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new variable 'has_dermo' based on the condition\n",
    "merged_df['has_dermo'] = (merged_df['Disease_Scale'] != 0).astype(int)\n",
    "# Count the number of observations with 'dermo'\n",
    "dermo_count = merged_df['has_dermo'].sum()\n",
    "\n",
    "print(dermo_count)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features and target\n",
    "features = [ 'Oyster_Number', 'Height_mm', 'Length_mm', 'Width_mm', 'Total_Weight_g',\n",
    "            'Soft_Tiss_Wet', 'Shell_Wet', 'Soft_Tiss_Dry', 'Shell_Dry', 'POINT_X', 'POINT_Y',\n",
    "            'Ambient_Temp_°C', 'Water_Temp_°C', 'Salinity_PPT', 'SiteID_encoded', 'Sex_encoded']\n",
    "\n",
    "target = 'has_dermo'\n",
    "\n",
    "X = merged_df[features]\n",
    "y = merged_df[target]\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')  # replace nan with mean\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# train test split: use 80-20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# make decision tree\n",
    "max_depth_value = 5  # implementing max depth for second iteration\n",
    "model = DecisionTreeClassifier(max_depth=max_depth_value, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# evalutation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", classification_rep)\n",
    "\n",
    "# visualize tree\n",
    "plt.figure(figsize=(12, 8))\n",
    "tree.plot_tree(model, feature_names=features, class_names=['Not Hit', 'Hit'], filled=True, rounded=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Importance for Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from the model\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Create a dataframe with feature names and their importances\n",
    "feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
    "\n",
    "# Sort the dataframe by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df, palette='viridis')\n",
    "plt.title('Feature Importances in Decision Tree Classifier')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Net Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# create a neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(30, activation='relu', input_dim=X_train_scaled.shape[1]))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# evaluate the model on the test set\n",
    "y_pred_prob = model.predict(X_test_scaled)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizing Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimize_neural_network(X, y, param_grid, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Optimize parameters for a neural network classifier using grid search.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Features\n",
    "    - y: Target variable\n",
    "    - param_grid: Dictionary of hyperparameters for grid search\n",
    "    - test_size: Proportion of the dataset to include in the test split\n",
    "    - random_state: Seed for random number generation\n",
    "\n",
    "    Returns:\n",
    "    - Best parameters found by grid search\n",
    "    - Best model with the optimal parameters\n",
    "    - Classification report on the test set using the best model\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # create a neural network classifier\n",
    "    neural_net = MLPClassifier()\n",
    "\n",
    "    # create a GridSearchCV object\n",
    "    grid_search = GridSearchCV(neural_net, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "    # fit the grid search to the data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # get the best parameters and the corresponding best model\n",
    "    best_params = grid_search.best_params_\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # evaluate the best model on the test set\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    return best_params, best_model, report\n",
    "\n",
    "# usage:\n",
    "param_grid = {'hidden_layer_sizes': [(10,), (50,), (100,)], 'alpha': [0.0001, 0.001, 0.01]}\n",
    "best_params, best_model, classification_report = optimize_neural_network(X, y, param_grid)\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Classification Report:\\n\", classification_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Gaussian Naive Bayes classifier\n",
    "naive_bayes = GaussianNB()\n",
    "\n",
    "# train\n",
    "naive_bayes.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = naive_bayes.predict(X_test)\n",
    "\n",
    "# evaluate\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaN values in 'disease_scale' and drop those rows\n",
    "merged_df = merged_df.dropna(subset=['Disease_Scale'])\n",
    "\n",
    "# convert 'disease_scale' to numeric \n",
    "merged_df['Disease_Scale'] = pd.to_numeric(merged_df['Disease_Scale'], errors='coerce')\n",
    "\n",
    "# create a base map centered around the average latitude and longitude\n",
    "average_lat = merged_df['POINT_Y'].mean()\n",
    "average_lon = merged_df['POINT_X'].mean()\n",
    "mymap = folium.Map(location=[average_lat, average_lon], zoom_start=10)\n",
    "\n",
    "# create a HeatMap layer\n",
    "heat_data = [[row['POINT_Y'], row['POINT_X'], row['Disease_Scale']] for index, row in merged_df.iterrows()]\n",
    "HeatMap(heat_data, name=\"Disease Scale\").add_to(mymap)\n",
    "\n",
    "# display the map\n",
    "mymap.save(\"disease_map.html\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
